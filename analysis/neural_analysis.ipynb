{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import math\n",
    "import nibabel as nb\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load in all the subjects .nii files. Then, import the CSV file that specifies the timestamps where internal monologue is present during each video. For each video, the subject's neural timeseries file will be segmented out into 25 clips. This means that altogether, each subject will have 50 clips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhdpath = '/Volumes/LT/phd/inarr/'\n",
    "sublist = [x for x in os.listdir(xhdpath) if 'sub' in x]\n",
    "\n",
    "timestamps = pd.read_csv('/Users/lindseytepfer/Documents/phd/inarr/monologue_timestamps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stutterer shape: (77, 95, 82, 373)\n",
    "#physical shape: (77, 95, 82, 302)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in each .nii image and the subsetted dataframe, and uses the timestamps inside of the dataframe to determine where the nii files must be trimmed. Because we're dealing with brain data, we shift the timecourse by 6 seconds to account for the hemodynamic response function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_image(image, df, snum, movie):\n",
    "\n",
    "    for i in range(df.index.min(),df.index.max()+1):\n",
    "\n",
    "        start = math.floor((df.loc[i, \"start\"] / 2) + 6)\n",
    "        stop = math.ceil((df.loc[i, \"stop\"] /2) + 6)\n",
    "        type = df.loc[i, \"type\"]\n",
    "        version = df.loc[i, \"version\"]\n",
    "        \n",
    "        if i != df.index[-1]:\n",
    "            sub_trimmed = image.slicer[:,:,:,start:stop]\n",
    "            nb.save(sub_trimmed,xhdpath+snum+\"/segmented_files/\"+snum+\"_\"+movie+\"_clip-\"+str(i)+\"_\"+type+\"_version-\"+str(version)+\".nii.gz\")\n",
    "        \n",
    "        else:\n",
    "            sub_trimmed = image.slicer[:,:,:,start:]\n",
    "            nb.save(sub_trimmed,xhdpath+snum+\"/segmented_files/\"+snum+\"_\"+movie+\"_\"+\"_clip-\"+str(i)+\"_\"+type+\"_version-\"+str(version)+\".nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sublist:\n",
    "    print(sub)\n",
    "    sub_files = [x for x in os.listdir(xhdpath+sub) if '.nii.gz' in x]\n",
    "\n",
    "    for f in sub_files:\n",
    "        sub_brain = nb.load(xhdpath+sub+os.sep+f)\n",
    "\n",
    "        if 'physical' in f:\n",
    "            phys_df = timestamps[(timestamps.movie == 'physical')].copy()\n",
    "            trim_image(sub_brain, phys_df, sub, 'physical')\n",
    "\n",
    "        elif 'stutterer' in f:\n",
    "            stut_df = timestamps[(timestamps.movie == 'stutterer')].copy().reset_index()\n",
    "            trim_image(sub_brain, stut_df, sub, 'stutterer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='hotpink'> Univariate analysis </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we mask each clip into 400 different sub-regions (parcels) of the brain. In doing so, we create a 400-row dataframe for each clip, where each row is a parcel, and the column lengths are variable dependent upon the size of a given parcel. \n",
    "\n",
    "Afterwards, for each participant, we take the parcel data related to the internal monologue segments, and compute the mean for each clip. \n",
    "\n",
    "Take the clips from the (we ignore the non-internal monologue moments for this analysis) and take the silenced or unsilenced and you can compute a mean for each one of those clips, averaging both voxels within a parcel, and across time. We get 12 values per participant per parcel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "from nilearn import datasets\n",
    "import nilearn.image as image\n",
    "from nilearn.maskers import NiftiMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "                                                    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "'''\n",
    "From the documentation:\n",
    "The list of labels does not contain ‘Background’ by default. \n",
    "To have proper indexing, you should either manually add ‘Background’ to the list of labels:\n",
    "'''\n",
    "\n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holding all 400 parcel masks in memory; takes apprx 2m13s\n",
    "mask_list = []\n",
    "\n",
    "for p in range(1,402): #402\n",
    "\n",
    "    try:\n",
    "        # create a new image from the parcel coordinate data \n",
    "        parcel = nilearn.image.new_img_like(schaefer_atlas.maps, nilearn.image.get_data(schaefer_atlas.maps) == p) #hold the parcel masks in memory \n",
    "        \n",
    "        #convert the image into a mask\n",
    "        masker = NiftiMasker() \n",
    "        parcel_mask = masker.fit(parcel)\n",
    "\n",
    "        #throw the mask into a list\n",
    "        mask_list.append(parcel_mask)\n",
    "    \n",
    "    except: \n",
    "        print(\"out of range, p=\", p)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix,mask in enumerate(mask_list):\n",
    "    print(ix)\n",
    "    parcel_data = []\n",
    "    subidlist, movielist, cliplist, versionlist, typelist = [], [], [], [], []\n",
    "    \n",
    "    for sub in sublist:\n",
    "        print(sub)\n",
    "\n",
    "        sub_clips = [x for x in os.listdir(xhdpath+sub+'/segmented_files/') if '_IM_' in x]\n",
    "        sub_clips.sort()\n",
    "    \n",
    "        for clip in sub_clips:\n",
    "            movie = clip.split('_')[1]\n",
    "            clip_num = clip.split('_')[2]\n",
    "            type = clip.split('_')[3]\n",
    "            version = clip.split('_')[4].split('.')[0]\n",
    "\n",
    "            clip_segment = nb.load(xhdpath+sub+'/segmented_files/'+clip)\n",
    "            #this function takes the image's mean over time (the 4th dimension)\n",
    "            clip_avg = image.mean_img(clip_segment)\n",
    "\n",
    "            try:\n",
    "                roi_data = mask.transform_single_imgs(clip_avg)\n",
    "                parcel_data.append(roi_data[0])\n",
    "\n",
    "                subidlist.append(sub)\n",
    "                movielist.append(movie)\n",
    "                cliplist.append(clip_num)\n",
    "                versionlist.append(version.split('-')[1])\n",
    "                typelist.append(type)\n",
    "            except:\n",
    "                print(\"index:\", ix, sub, clip)\n",
    "                \n",
    "    df = pd.DataFrame(parcel_data)\n",
    "    df['sub'] = subidlist\n",
    "    df['movie'] = movielist\n",
    "    df['clip'] = cliplist\n",
    "    df['type'] = typelist\n",
    "    df['version'] = versionlist\n",
    "    \n",
    "    df.to_csv(xhdpath+\"/univariate_parcels/parcel_\"+str(ix)+\".csv\", index=False)\n",
    "    print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I average the voxels of each parcel, creating a reduced dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_paths = '/Volumes/Scraplab/lindseytepfer/inarr/univariate_parcels/'\n",
    "parcels = [x for x in os.listdir(parcel_paths) if '.csv' in x] #400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in parcels:\n",
    "    df = pd.read_csv(parcel_paths+i)\n",
    "    ignore_columns = ['sub', 'movie', 'clip', 'type', 'version']\n",
    "    voxel_columns = [x for x in df.columns if x not in ignore_columns]\n",
    "    df['avg_column'] = df[voxel_columns].mean(axis=1)\n",
    "    new_df = df[['avg_column', 'sub', 'movie', 'clip', 'type', 'version']]\n",
    "    new_df.to_csv(parcel_paths+\"averaged/\"+i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Univariate Result\n",
    "Now that we have all of our t-values, we insert them into a brain image and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets, plotting, surface\n",
    "import nibabel as nb\n",
    "from nilearn.plotting import plot_img_on_surf\n",
    "\n",
    "fsaverage = datasets.fetch_surf_fsaverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "\n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tval_path = \"/Volumes/rc/lab/S/Scraplab/lindseytepfer/inarr/univariate_parcels/averaged/\"\n",
    "t_vals = []\n",
    "\n",
    "# for i in range(400):\n",
    "#     df = pd.read_csv(tval_path+\"t_value_parcel_\"+str(i)+\".csv\")\n",
    "#     t_vals.append(df['t_value'][0])\n",
    "\n",
    "# tval_df = pd.DataFrame(t_maps, columns=[\"t_values\"])\n",
    "# tval_df.to_csv(tmap_path+\"tvalues_dataframe.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(tmap_path+\"tvalues_dataframe.csv\")\n",
    "t_val_list = list(df[\"t_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas = nb.load(schaefer_atlas.maps) # .maps provides the path to the map\n",
    "atlas_data = atlas.get_fdata()\n",
    "affine = atlas.affine\n",
    "\n",
    "mapped_data = np.zeros_like(atlas_data)\n",
    "mapped_data[atlas_data == 0] = np.nan # mark\n",
    "unique_regions = np.unique(atlas_data)[1:]\n",
    "\n",
    "for i, region in enumerate(unique_regions):\n",
    "    mapped_data[atlas_data == region] = -np.log10(t_val_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tval_img = nb.Nifti1Image(mapped_data, affine)\n",
    "nb.save(tval_img, 'tvalue_map.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_img_on_surf(\n",
    "    tval_img,\"fsaverage\", inflate=True,\n",
    "    views=['lateral', 'medial'],  # Hemispheric views to display\n",
    "    hemispheres=['left', 'right'],  # Both hemispheres\n",
    "    threshold=0.05,  # Highlight significant p-values\n",
    "    cmap='plasma',  # Colormap (e.g., 'viridis', 'coolwarm')\n",
    "    colorbar=True,  # Show colorbar\n",
    "    vmin=0, vmax=.4,\n",
    ")\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='turquoise'> Multivariate analysis (ISC)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again want to mask out our clips, but this time we want on the the NIM set of clips - the _non-internal monologue segments_, and we want the voxel-wise average across time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "from nilearn import datasets\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "                                                    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "'''\n",
    "From the documentation:\n",
    "The list of labels does not contain ‘Background’ by default. \n",
    "To have proper indexing, you should either manually add ‘Background’ to the list of labels:\n",
    "'''\n",
    "\n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")\n",
    "\n",
    "masker = NiftiLabelsMasker(\n",
    "    labels_img=schaefer_atlas.maps,\n",
    "    strategy='mean',  # Averages voxels in parcel at each TR\n",
    "    standardize=False  # don't want z-scoring\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sublist[0:1]:\n",
    "    sub_clips = [x for x in os.listdir(xhdpath+sub+'/segmented_files/') if '_NIM_' in x]\n",
    "    sub_clips = [x for x in sub_clips if 'clip-0' not in x] #discard the first clip\n",
    "    sub_clips.sort()\n",
    "\n",
    "    for clip in sub_clips[0:1]:\n",
    "        subidlist, movielist, cliplist, versionlist, typelist = [], [], [], [], []\n",
    "        movie = clip.split('_')[1]\n",
    "        clip_num = clip.split('_')[2]\n",
    "        type = clip.split('_')[3]\n",
    "        version = clip.split('_')[4].split('.')[0]\n",
    "\n",
    "        clip_segment = nb.load(xhdpath+sub+'/segmented_files/'+clip)\n",
    "        #this function takes the image's mean over time (the 4th dimension)\n",
    "        time_by_parcel = masker.fit_transform(clip_segment)\n",
    "\n",
    "        subidlist.append(sub)\n",
    "        movielist.append(movie)\n",
    "        cliplist.append(clip_num)\n",
    "\n",
    "        df = pd.DataFrame(time_by_parcel)\n",
    "\n",
    "        df['sub'] = len(df)*subidlist\n",
    "        df['movie'] = len(df)*movielist\n",
    "        df['clip'] = len(df)*cliplist\n",
    "        \n",
    "        df.to_csv(\"/Volumes/Scraplab/lindseytepfer/inarr/isc_parcels/\"+sub+\"_\"+movie+\"_\"+clip_num+\".csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_volume = \"/Volumes/rc/lab/S/Scraplab/lindseytepfer/inarr/isc_parcels/\"\n",
    "\n",
    "v1_subs = ['sub-1','sub-10','sub-13','sub-14','sub-17','sub-18','sub-19','sub-20','sub-21','sub-24', 'sub-25','sub-26', 'sub-27']\n",
    "v2_subs = ['sub-2','sub-3','sub-4','sub-5','sub-6','sub-7','sub-8','sub-9','sub-11','sub-12','sub-15','sub-16','sub-22','sub-23']\n",
    "sublist = v1_subs + v2_subs\n",
    "\n",
    "#ignore clip-0\n",
    "clip_list = ['clip-2','clip-4','clip-6','clip-8','clip-10','clip-12',\n",
    "             'clip-14','clip-16','clip-18','clip-20','clip-22','clip-24']\n",
    "\n",
    "movie_list = [\"stutterer\", \"physical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v1'\n",
    "file_list = [x for x in os.listdir(scrap_volume+version) if '.csv' in x]\n",
    "\n",
    "for parcel in range(400)[1:]:\n",
    "    for movie in movie_list:\n",
    "        sub_list = [x for x in file_list if movie in x]\n",
    "\n",
    "        for clip in clip_list:\n",
    "\n",
    "            filtered_files = [x for x in sub_list if clip+\".csv\" in x] #eg, clip-2\n",
    "\n",
    "            all_subs_list = []\n",
    "\n",
    "            for f in filtered_files:\n",
    "                fname_split = f.split(\"_\")\n",
    "                x = pd.read_csv(scrap_volume+version+os.sep+f, usecols=[str(parcel)])\n",
    "                x = x.T\n",
    "                x[['sub', 'movie']] = fname_split[0],fname_split[1]\n",
    "                all_subs_list.append(x)\n",
    "\n",
    "            df = pd.concat(all_subs_list)\n",
    "            df.sort_values(by='sub', inplace=True)\n",
    "\n",
    "            df.to_csv(scrap_volume+version+os.sep+\"parcels/parcel_\"+str(parcel)+\"_\"+movie+\"_\"+clip+\".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our .csv files have been re-organized, we can correlate each parcelxclip dataframe and then take the averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_path = '/Volumes/Scraplab/lindseytepfer/inarr/isc_parcels/v1/parcels/'\n",
    "parcel_files = [x for x in os.listdir(parcel_path) if '.csv' in x] #all parcelxclips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this on andes tomorrow\n",
    "\n",
    "for movie in movie_list:\n",
    "\n",
    "    parcel_list = [x for x in parcel_files if movie in x]\n",
    "    parcel_avg_corr = []\n",
    "\n",
    "    for p in parcel_list:\n",
    "        pnum = p.split(\"_\"+movie)[0]\n",
    "        filt_list = [x for x in parcel_list if pnum in x]\n",
    "        filt_list = [x for x in filt_list if 'clip-0' not in x]\n",
    "\n",
    "        parcel_clip_corr_list = []\n",
    "\n",
    "        for ix,clip in enumerate(filt_list): #always 12 clips\n",
    "            fname = clip.split(\"_\")\n",
    "            df = pd.read_csv(parcel_path+clip)\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "            timecourses = df.drop(['sub', 'movie'], axis=1)\n",
    "            parcel_clip_corr_list.append(np.corrcoef(timecourses)) #(13,13) we currently have 13 subjects in v1\n",
    "\n",
    "        parcel_clip_arr = np.array(parcel_clip_corr_list)\n",
    "        parcel_clip_avg = np.mean(parcel_clip_arr, axis=0)\n",
    "        out_path = f\"/Volumes/Scraplab/lindseytepfer/inarr/isc_parcels/v1/averages/{pnum}_{movie}_avg\"\n",
    "        np.save(out_path,parcel_clip_avg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we get the difference between the two versions across the parcels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we permute our version assignments so that we can perform significance testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='purple'>ISC to Behavioral Analysis</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import math\n",
    "import nibabel as nb\n",
    "import numpy as np\n",
    "\n",
    "import nilearn\n",
    "from nilearn import image, signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load in all the subjects .nii files. Then, import the CSV file that specifies the timestamps where internal monologue is present during each video. For each video, the subject's neural timeseries file will be segmented out into 25 clips. This means that altogether, each subject will have 50 clips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Volumes/Scraplab/fSEND/inarr_data/'\n",
    "\n",
    "sublist = [x for x in os.listdir(data_path) if 'sub' in x]\n",
    "\n",
    "timestamps = pd.read_csv(data_path+'monologue_timestamps.csv')\n",
    "\n",
    "v1_subs = ['sub-1','sub-10','sub-13','sub-14','sub-17','sub-18','sub-19',\n",
    "'sub-20','sub-21','sub-24','sub-25','sub-26','sub-27']\n",
    "v2_subs = ['sub-2','sub-3','sub-4','sub-5','sub-6','sub-7','sub-8','sub-9',\n",
    "'sub-11','sub-12','sub-15','sub-16','sub-22','sub-23', 'sub-28']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in each .nii image and the subsetted dataframe, and uses the timestamps inside of the dataframe to determine where the nii files must be trimmed. Because we're dealing with brain data, we shift the timecourse by 6 seconds to account for the hemodynamic response function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_image(brain, df, snum, movie):\n",
    "\n",
    "    for i in range(df.index.min(),df.index.max()+1):\n",
    "\n",
    "        # 6-second HRF \n",
    "        start = math.floor((df.loc[i, \"start\"] / 2) + 6)\n",
    "        stop = math.ceil((df.loc[i, \"stop\"] /2) + 6)\n",
    "        type = df.loc[i, \"type\"]\n",
    "        version_silenced = df.loc[i, \"version_silenced\"]\n",
    "\n",
    "        if snum in v1_subs:\n",
    "            version = 1\n",
    "        elif snum in v2_subs:\n",
    "            version = 2\n",
    "\n",
    "        if i%2 == 1: #this means its an IM segment\n",
    "            if version_silenced == version:\n",
    "                version = str(version)+\"_silenced\"\n",
    "\n",
    "        if i != df.index[-1]:\n",
    "            sub_trimmed = brain.slicer[:,:,:,start:stop]\n",
    "        else:\n",
    "            sub_trimmed = brain.slicer[:,:,:,start:]\n",
    "\n",
    "        nb.save(sub_trimmed,data_path+snum+\"/segmented_files/\"+snum+\"_\"+movie+\"_clip-\"+str(i)+\"_\"+type+\"_version-\"+str(version)+\".nii.gz\")\n",
    "    \n",
    "    return (print(\"trimming complete.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in sublist[1:]:\n",
    "    print(sub)\n",
    "    sub_files = [x for x in os.listdir(data_path+sub) if '.nii.gz' in x]\n",
    "    confounds = [x for x in os.listdir(data_path+sub) if '.tsv' in x]\n",
    "\n",
    "    for f in sub_files:\n",
    "\n",
    "        if 'physical' in f:\n",
    "            movie = 'physical'\n",
    "        elif 'stutterer' in f:\n",
    "            movie = 'stutterer'\n",
    "        \n",
    "        #take this step to regress out confounds -- head motion, csf, white matter, etc\n",
    "        conf_file = f'{sub}_task-{movie}_desc-confounds_timeseries.tsv'\n",
    "\n",
    "        bold = nb.load(data_path+sub+os.sep+f)\n",
    "        n_scans = bold.shape[-1]\n",
    "        time = np.arange(n_scans)\n",
    "        \n",
    "        # Create polynomial trends (linear and quadratic)\n",
    "        linear_trend = time\n",
    "        quadratic_trend = time**2\n",
    "\n",
    "        conf = pd.read_csv(data_path+sub+os.sep+conf_file, sep='\\t', header=0)\n",
    "        confounds = [c for c in conf.columns if not '_comp' in c]\n",
    "        confounds = [c for c in confounds if not 'cosine' in c]\n",
    "        confounds = [c for c in confounds if not 'motion' in c]\n",
    "        conf = conf.loc[:,confounds]\n",
    "\n",
    "        poly_confounds = np.column_stack((linear_trend, quadratic_trend, conf))\n",
    "        poly_confounds[np.isnan(poly_confounds)] = 0\n",
    "\n",
    "        cleaned_data = signal.clean(bold.get_fdata().reshape(-1, n_scans).T,confounds=poly_confounds,standardize='zscore_sample')\n",
    "        clean_bold = image.new_img_like(bold, cleaned_data.T.reshape(bold.shape))\n",
    "\n",
    "        if movie == 'physical':\n",
    "            phys_df = timestamps[(timestamps.movie == 'physical')].copy()\n",
    "            trim_image(clean_bold, phys_df, sub, 'physical')\n",
    "\n",
    "        elif movie == 'stutterer':\n",
    "            stut_df = timestamps[(timestamps.movie == 'stutterer')].copy().reset_index()\n",
    "            trim_image(clean_bold, stut_df, sub, 'stutterer')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

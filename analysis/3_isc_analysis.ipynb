{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='goldenrod'> Inter-subject Correlation Analyses</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Brain packages\n",
    "import nibabel as nb\n",
    "import nilearn\n",
    "from nilearn import datasets\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/lindseytepfer/nilearn_data/schaefer_2018\n"
     ]
    }
   ],
   "source": [
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "                                                    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "'''\n",
    "From the documentation:\n",
    "The list of labels does not contain ‘Background’ by default. \n",
    "To have proper indexing, you should either manually add ‘Background’ to the list of labels:\n",
    "'''\n",
    "\n",
    "masker = NiftiLabelsMasker(\n",
    "    labels_img=schaefer_atlas.maps,\n",
    "    strategy='mean',  # Averages voxels in parcel at each TR\n",
    "    standardize=False  # don't want z-scoring\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we prapre for the ISC analysis by aggregating the segmented clip data across all 28 subjects into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_clip_path = '/Volumes/Scraplab/fSEND/inarr_data/'\n",
    "sublist = [x for x in os.listdir(segmented_clip_path) if 'sub' in x]\n",
    "complete_cliplist = []\n",
    "\n",
    "#compile all of the clip files across subjects\n",
    "for sub in sublist:\n",
    "    try:\n",
    "        segment_list = [x for x in os.listdir(segmented_clip_path+sub+os.sep+\"segmented_files/\") if \"_NIM\" in x]\n",
    "        discard_first_clip = [x for x in segment_list if \"_clip-0_\" not in x]\n",
    "\n",
    "        for f in discard_first_clip:\n",
    "            complete_cliplist.append(f)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "movie_list = ['physical', 'stutterer']\n",
    "\n",
    "#for NIM, we don't include the very first clip, clip-0:\n",
    "nim_clips = ['clip-2','clip-4','clip-6','clip-8','clip-10','clip-12',\n",
    "             'clip-14','clip-16','clip-18','clip-20','clip-22','clip-24']\n",
    "\n",
    "im_clips = ['clip-1','clip-3','clip-5','clip-7','clip-9','clip-11','clip-13',\n",
    "             'clip-15','clip-17','clip-19','clip-21','clip-23']\n",
    "\n",
    "version_matrix = np.loadtxt(\"version_matrix.csv\",delimiter=\",\", dtype=int, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='goldenrod'>Calculate isc matrices</font>\n",
    "\n",
    "Once we have a list of all 28 subjects neural clip segments, for both movies, we calculate the inter-subject correlation for the NIM clip segments, and the IM clip segments. This produces two ISC matrices--one for the IM, and one for NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_nim_matrix = np.empty([2, 12, 400, 28, 28])\n",
    "isc_im_matrix = np.empty([2, 12, 400, 28, 28])\n",
    "\n",
    "for movie in movie_list[0:1]:\n",
    "    #all the subject's clips corresponding to stutterer, for example\n",
    "    movie_cliplist = [x for x in complete_cliplist if movie in x]\n",
    "\n",
    "    clip_to_version_correlation = np.zeros([12,400]) #NIM is 12 clips without the first one\n",
    "\n",
    "    for ix, i in enumerate(nim_clips[0:1]):\n",
    "\n",
    "        #all 28 subjects clip-1's for example\n",
    "        filtered_clips = [x for x in movie_cliplist if i+\"_\" in x]\n",
    "\n",
    "        #doing this ensures that my resulting matrix is in the *same exact order* as our version matrix\n",
    "        v1_clips = natsorted([x for x in filtered_clips if 'version-1' in x])\n",
    "        v2_clips = natsorted([x for x in filtered_clips if 'version-2' in x])\n",
    "\n",
    "        single_clip_matrix_list = [] #this will be len(28) for each subject!\n",
    "        preceding_clip_matrix_list = []\n",
    "\n",
    "        for clip in v1_clips:\n",
    "\n",
    "            sub = clip.split(\"_\")[0]\n",
    "            clip_segment_nii = nb.load(segmented_clip_path+sub+'/segmented_files/'+clip)\n",
    "            preceding_clip = glob.glob(segmented_clip_path+sub+'/segmented_files/'+sub+\"_\"+movie+\"_\"+im_clips[ix]+\"_IM_version-1*.nii.gz\")[0]\n",
    "            preceding_clip_nii = nb.load(preceding_clip)\n",
    "\n",
    "            \"\"\" \n",
    "            masker.fit_transform(clip_segment_nii) will take the voxelwise mean (ie it preserves\n",
    "            timecourse information). This produces a timepoint x parcel csv, eg for physical's \n",
    "            clip_0, it will generate a (74,40) shape.\n",
    "            \"\"\"\n",
    "\n",
    "            time_by_parcel = masker.fit_transform(clip_segment_nii) #producess a timepoint x parcel csv, eg (2, 400)\n",
    "            time_by_parcel_preceding_clip = masker.fit_transform(preceding_clip_nii)\n",
    "\n",
    "            single_clip_matrix_list.append(time_by_parcel)\n",
    "            preceding_clip_matrix_list.append(time_by_parcel_preceding_clip)\n",
    "        \n",
    "        for clip in v2_clips:\n",
    "\n",
    "            sub = clip.split(\"_\")[0]\n",
    "            clip_segment_nii = nb.load(segmented_clip_path+sub+'/segmented_files/'+clip)\n",
    "            preceding_clip = glob.glob(segmented_clip_path+sub+'/segmented_files/'+sub+\"_\"+movie+\"_\"+im_clips[ix]+\"_IM_version-2*.nii.gz\")[0]\n",
    "            preceding_clip_nii = nb.load(preceding_clip)\n",
    "\n",
    "            time_by_parcel = masker.fit_transform(clip_segment_nii) #producess a timepoint x parcel csv, eg (2, 400)\n",
    "            time_by_parcel_preceding_clip = masker.fit_transform(preceding_clip_nii)\n",
    "\n",
    "            single_clip_matrix_list.append(time_by_parcel)\n",
    "            preceding_clip_matrix_list.append(time_by_parcel_preceding_clip)\n",
    "        \n",
    "        single_clip_mat_arr = np.array(single_clip_matrix_list) #shape is (28, TR, 400) so for clip-2 its: (28,14,400)\n",
    "        preceding_clip_mat_arr = np.array(preceding_clip_matrix_list)\n",
    "\n",
    "        for p in range(400)[0:1]:\n",
    "            sub_clip_corr = np.corrcoef(single_clip_mat_arr[:,:,p]) #(28,28)\n",
    "            im_clips_corr = np.corrcoef(preceding_clip_mat_arr[:,:,p]) #also (28,28)\n",
    "            \n",
    "            isc_nim_matrix[mx, cx, p] = sub_clip_corr\n",
    "            isc_im_matrix[mx, cx, p] = im_clips_corr\n",
    "\n",
    "np.save('isc_nim_matrix', isc_nim_matrix) #shape: (2, 12, 400, 28, 28)\n",
    "np.save('isc_im_matrix', isc_im_matrix) #shape: (2, 12, 400, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='goldenrod'> Version correlation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Monologues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_im_matrix = np.load(\"outputs/isc/isc_im_matrix.npy\")\n",
    "\n",
    "im_isc_result = np.empty((2,12,400))\n",
    "\n",
    "for mx in range(2):\n",
    "    for cx in range(12):\n",
    "        for p in range(400):\n",
    "\n",
    "            im_neural_corr = isc_im_matrix[mx,cx,p] #(28,28)\n",
    "\n",
    "            #correlate shuffled IM ISC matrix with the version matrix\n",
    "            neural_lower = nilearn.connectome.sym_matrix_to_vec(im_neural_corr, discard_diagonal=True)\n",
    "            version_lower = nilearn.connectome.sym_matrix_to_vec(version_matrix, discard_diagonal=True)\n",
    "\n",
    "            im_isc_result[mx,cx,p] = np.corrcoef(neural_lower, version_lower)[0][1]\n",
    "\n",
    "\n",
    "#average across clips and movies\n",
    "im_isc_result = im_isc_result.mean(axis=(0,1)) #shape (400)\n",
    "np.save('outputs/isc/im_isc_result', im_isc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-internal monologues\n",
    "\n",
    "For the NIM ISC, we add one additional step: controlling for the IM signal within the NIM matrix. This is so that we can ensure our analyses only reflect the ISC specific to the NIM scenes, without the contamination of the IM scene that occurred immediately before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_nim_matrix = np.load(\"outputs/isc/isc_nim_matrix.npy\")\n",
    "isc_im_matrix = np.load(\"outputs/isc/isc_im_matrix.npy\")\n",
    "\n",
    "isc_nim_cleaned = np.empty((2,12,400))\n",
    "nim_residuals = np.empty([2,12,400,28, 28])\n",
    "\n",
    "for mx in range(2):\n",
    "    for cx in range(12):\n",
    "        for p in range(400):\n",
    "            \n",
    "            nim_neural_corr = isc_nim_matrix[mx,cx,p] #(28,28)\n",
    "            im_neural_corr = isc_im_matrix[mx,cx,p] #(28,28)\n",
    "\n",
    "            #regress out the IM signal \n",
    "            nim_flat, im_flat = nim_neural_corr.flatten(), im_neural_corr.flatten()\n",
    "            im_const = sm.add_constant(im_flat)\n",
    "            model = sm.OLS(nim_flat, im_const)\n",
    "            results = model.fit()\n",
    "            residual_signal = results.resid\n",
    "            residual_reshaped = residual_signal.reshape(28, 28)\n",
    "            nim_residuals[mx, cx,p] = residual_reshaped\n",
    "\n",
    "            #correlate cleaned NIM signal with the version matrix\n",
    "            neural_lower = nilearn.connectome.sym_matrix_to_vec(residual_reshaped, discard_diagonal=True)\n",
    "            version_lower = nilearn.connectome.sym_matrix_to_vec(version_matrix, discard_diagonal=True)\n",
    "\n",
    "            result = np.corrcoef(neural_lower, version_lower)[0][1]\n",
    "\n",
    "            isc_nim_cleaned[mx,cx,p] = result\n",
    "\n",
    "isc_nim_cleaned = isc_nim_cleaned.mean(axis=(0,1)) #shape (400)\n",
    "np.save('outputs/isc/isc_nim_cleaned_matrix', isc_nim_cleaned) #(2,12,400)\n",
    "np.save('outputs/isc/isc_nim_residuals_matrix', nim_residuals) #(2, 12, 400, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='goldenrod'>Permutation Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner monologues permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_im_matrix = np.load(\"outputs/isc/isc_im_matrix.npy\")\n",
    "\n",
    "np.random.seed(0)\n",
    "im_permutation_matrix = np.empty((2,12,400,5000))\n",
    "\n",
    "for perm in range(1000):\n",
    "    random_array = np.random.permutation(28)\n",
    "\n",
    "    for mx in range(2):\n",
    "        for cx in range(12):\n",
    "            for p in range(400):\n",
    "\n",
    "                im_neural_corr = isc_im_matrix[mx,cx,p] #(28,28)\n",
    "                shuffled_matrix = im_neural_corr[random_array][:, random_array]\n",
    "\n",
    "                #correlate shuffled IM ISC matrix with the version matrix\n",
    "                neural_lower = nilearn.connectome.sym_matrix_to_vec(shuffled_matrix, discard_diagonal=True)\n",
    "                version_lower = nilearn.connectome.sym_matrix_to_vec(version_matrix, discard_diagonal=True)\n",
    "\n",
    "                im_permutation_matrix[mx,cx,p,perm] = np.corrcoef(neural_lower, version_lower)[0][1]\n",
    "\n",
    "\n",
    "#average across clips and movies\n",
    "im_permutation_matrix = im_permutation_matrix.mean(axis=(0,1)) #shape (400,5000)\n",
    "np.save('outputs/isc/im_permutation_matrix', im_permutation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-internal monologues permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_nim_residuals_matrix = np.load(\"outputs/isc/isc_nim_residuals_matrix.npy\") #(2, 12, 400, 28, 28)\n",
    "\n",
    "np.random.seed(0)\n",
    "nim_permutation_matrix = np.empty((2,12,400,5000))\n",
    "\n",
    "for perm in range(5000):\n",
    "    random_array = np.random.permutation(28)\n",
    "\n",
    "    for mx in range(2):\n",
    "        for cx in range(12):\n",
    "            for p in range(400):\n",
    "\n",
    "                nim_residuals = isc_nim_residuals_matrix[mx,cx,p] #(28,28)\n",
    "                shuffled_matrix = nim_residuals[random_array][:, random_array]\n",
    "\n",
    "                #correlate cleaned NIM signal with the version matrix\n",
    "                neural_lower = nilearn.connectome.sym_matrix_to_vec(shuffled_matrix, discard_diagonal=True)\n",
    "                version_lower = nilearn.connectome.sym_matrix_to_vec(version_matrix, discard_diagonal=True)\n",
    "\n",
    "                nim_permutation_matrix[mx,cx,p,perm] = np.corrcoef(neural_lower, version_lower)[0][1]\n",
    "\n",
    "\n",
    "#average across clips and movies\n",
    "nim_permutation_matrix = nim_permutation_matrix.mean(axis=(0,1)) #shape (400,5000)\n",
    "np.save('nim_permutation_matrix', nim_permutation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='goldenrod'>Results</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "from nilearn import plotting, datasets, image, surface\n",
    "import nibabel as nb\n",
    "from nilearn.plotting import plot_img_on_surf\n",
    "from nilearn.image import new_img_like\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fsaverage = datasets.fetch_surf_fsaverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal Monologues ISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_isc_result = np.load(\"outputs/isc/im_isc_result.npy\")\n",
    "im_permutation_matrix = np.load(\"outputs/isc/im_permutation_matrix.npy\") #(400, 5000)\n",
    "\n",
    "permuted_vals = np.abs(im_permutation_matrix)\n",
    "null_dist = permuted_vals.max(axis=0) #shape (5000) \n",
    "\n",
    "results_pvals = np.empty((400))\n",
    "\n",
    "for parcel in range(im_isc_result.shape[0]): #400, for each parcel\n",
    "    results_pvals[parcel] = np.mean(null_dist >= abs(im_isc_result[parcel]))\n",
    "\n",
    "significant = np.where(results_pvals < 0.05, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/lindseytepfer/nilearn_data/schaefer_2018\n"
     ]
    }
   ],
   "source": [
    "#create correlation value image\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "\n",
    "#need to insert the 0 for proper indexing. \n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")\n",
    "\n",
    "atlas = nb.load(schaefer_atlas.maps) # .maps provides the path to the map\n",
    "atlas_data = atlas.get_fdata()\n",
    "\n",
    "mapped_data = np.zeros_like(atlas_data)\n",
    "mapped_data[atlas_data == 0] = np.nan\n",
    "unique_regions = np.unique(atlas_data)[1:]\n",
    "\n",
    "for i, region in enumerate(unique_regions):\n",
    "    mapped_data[atlas_data == region] = im_isc_result[i]\n",
    "\n",
    "im_results_img = nb.Nifti1Image(mapped_data, atlas.affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corrected p-value mask:\n",
    "n_rois = 399\n",
    "\n",
    "roi_data = image.get_data(atlas)  # (x,y,z) array with ROI labels\n",
    "significant_mask = np.zeros_like(roi_data, dtype=np.float32)\n",
    "\n",
    "for roi_label in range(1, n_rois+1):\n",
    "    if significant[roi_label-1] == 1:  # ROI labels start at 1\n",
    "        significant_mask[roi_data == roi_label] = 1  # Tag significant voxels\n",
    "\n",
    "mask_img = new_img_like(atlas, significant_mask)\n",
    "#nb.save(mask_img, \"im_significance_mask.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/lindseytepfer/nilearn_data/fsaverage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindseytepfer/miniconda3/lib/python3.11/site-packages/nilearn/surface/surface.py:540: RuntimeWarning: Mean of empty slice\n",
      "  texture = np.nanmean(all_samples, axis=2)\n",
      "/Users/lindseytepfer/miniconda3/lib/python3.11/site-packages/nilearn/surface/surface.py:540: RuntimeWarning: Mean of empty slice\n",
      "  texture = np.nanmean(all_samples, axis=2)\n"
     ]
    }
   ],
   "source": [
    "plotting.plot_img_on_surf(im_results_img,\n",
    "    \"fsaverage\", inflate=True,\n",
    "    views=['lateral', 'medial'],  # Hemispheric views to display\n",
    "    hemispheres=['left', 'right'],  # Both hemispheres\n",
    "    cmap='autumn',  # Colormap (e.g., 'viridis', 'coolwarm')\n",
    "    colorbar=True,  # Show colorbar\n",
    "    mask_img=mask_img,\n",
    "    vmin=0, vmax=0.29, \n",
    ")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('/Users/lindseytepfer/Documents/phd/inarr/figures/im_isc_result.png', dpi=300, facecolor='w', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Internal Monologues ISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_nim_matrix = np.load(\"outputs/isc/isc_nim_cleaned_matrix.npy\") #(400)\n",
    "nim_permutation_matrix = np.load(\"outputs/isc/nim_permutation_matrix.npy\") #(400, 5000)\n",
    "\n",
    "permuted_vals = np.abs(nim_permutation_matrix)\n",
    "null_dist = permuted_vals.max(axis=0) #shape (10000) \n",
    "\n",
    "results_pvals = np.empty((400))\n",
    "\n",
    "for parcel in range(isc_nim_matrix.shape[0]): #400, for each parcel\n",
    "    results_pvals[parcel] = np.mean(null_dist >= abs(isc_nim_matrix[parcel]))\n",
    "\n",
    "significant = np.where(results_pvals < 0.05, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/lindseytepfer/nilearn_data/schaefer_2018\n"
     ]
    }
   ],
   "source": [
    "#create correlation value image\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "\n",
    "#need to insert the 0 for proper indexing. \n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")\n",
    "\n",
    "atlas = nb.load(schaefer_atlas.maps) # .maps provides the path to the map\n",
    "atlas_data = atlas.get_fdata()\n",
    "\n",
    "mapped_data = np.zeros_like(atlas_data)\n",
    "mapped_data[atlas_data == 0] = np.nan # mark\n",
    "unique_regions = np.unique(atlas_data)[1:]\n",
    "\n",
    "for i, region in enumerate(unique_regions):\n",
    "    mapped_data[atlas_data == region] = isc_nim_matrix[i]\n",
    "\n",
    "nim_results_img = nb.Nifti1Image(mapped_data, atlas.affine)\n",
    "nb.save(nim_results_img, \"nim_results_img.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corrected p-value mask:\n",
    "n_rois = 399\n",
    "\n",
    "roi_data = image.get_data(atlas)  # (x,y,z) array with ROI labels\n",
    "significant_mask = np.zeros_like(roi_data, dtype=np.float32)\n",
    "\n",
    "for roi_label in range(1, n_rois+1):\n",
    "    if significant[roi_label-1] == 1:  # ROI labels start at 1\n",
    "        significant_mask[roi_data == roi_label] = 1  # Tag significant voxels\n",
    "\n",
    "mask_img = new_img_like(atlas, significant_mask)\n",
    "\n",
    "nb.save(mask_img, \"nim_significance_mask.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[get_dataset_dir] Dataset found in /Users/lindseytepfer/nilearn_data/fsaverage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lindseytepfer/miniconda3/lib/python3.11/site-packages/nilearn/surface/surface.py:540: RuntimeWarning: Mean of empty slice\n",
      "  texture = np.nanmean(all_samples, axis=2)\n",
      "/Users/lindseytepfer/miniconda3/lib/python3.11/site-packages/nilearn/surface/surface.py:540: RuntimeWarning: Mean of empty slice\n",
      "  texture = np.nanmean(all_samples, axis=2)\n"
     ]
    }
   ],
   "source": [
    "plotting.plot_img_on_surf(nim_results_img,\n",
    "    \"fsaverage\", inflate=True,\n",
    "    views=['lateral', 'medial'],  # Hemispheric views to display\n",
    "    hemispheres=['left', 'right'],  # Both hemispheres\n",
    "    cmap='autumn',  # Colormap (e.g., 'viridis', 'coolwarm')\n",
    "    colorbar=True,  # Show colorbar\n",
    "    mask_img=mask_img,\n",
    "    vmin=0, vmax=0.29, \n",
    ")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.savefig('/Users/lindseytepfer/Documents/phd/inarr/figures/nim_isc_result.png', dpi=300, facecolor='w', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23464418755781338 0.29384885343949557\n"
     ]
    }
   ],
   "source": [
    "print(np.max(isc_nim_matrix), np.max(im_isc_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Non-Internal Monologues ISC Analyses</font> \n",
    "This is a primary analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_clip_path = '/Volumes/Scraplab/fSEND/inarr_data/'\n",
    "sublist = [x for x in os.listdir(segmented_clip_path) if 'sub' in x]\n",
    "complete_cliplist = []\n",
    "\n",
    "#compile all of the clip files across subjects\n",
    "for sub in sublist:\n",
    "    try:\n",
    "        segment_list = [x for x in os.listdir(segmented_clip_path+sub+os.sep+\"segmented_files/\") if \"_NIM\" in x] # [ IM / NIM ]\n",
    "        discard_first_clip = [x for x in segment_list if \"_clip-0_\" not in x]\n",
    "\n",
    "        for f in discard_first_clip:\n",
    "            complete_cliplist.append(f)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "movie_list = ['physical', 'stutterer']\n",
    "\n",
    "#we don't include the very first clip, clip-0:\n",
    "nim_clips = ['clip-2','clip-4','clip-6','clip-8','clip-10','clip-12',\n",
    "             'clip-14','clip-16','clip-18','clip-20','clip-22','clip-24']\n",
    "\n",
    "im_clips = ['clip-1','clip-3','clip-5','clip-7','clip-9','clip-11','clip-13',\n",
    "             'clip-15','clip-17','clip-19','clip-21','clip-23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in movie_list:\n",
    "    #all the subject's clips corresponding to stutterer, for example\n",
    "    movie_cliplist = [x for x in complete_cliplist if movie in x]\n",
    "\n",
    "    clip_to_version_correlation = np.zeros([12,400]) #NIM is 12 clips without the first one\n",
    "\n",
    "    for ix, i in enumerate(nim_clips):\n",
    "        print(\"starting clip:\", i)\n",
    "\n",
    "        #all 28 subjects clip-1's for example\n",
    "        filtered_clips = [x for x in movie_cliplist if i+\"_\" in x]\n",
    "\n",
    "        #doing this ensures that my resulting matrix is in the *same exact order* as our version matrix\n",
    "        v1_clips = natsorted([x for x in filtered_clips if 'version-1' in x])\n",
    "        v2_clips = natsorted([x for x in filtered_clips if 'version-2' in x])\n",
    "\n",
    "        single_clip_matrix_list = [] #this will be len(28) for each subject!\n",
    "\n",
    "        for clip in v1_clips:\n",
    "\n",
    "            sub = clip.split(\"_\")[0]\n",
    "            clip_segment_nii = nb.load(segmented_clip_path+sub+'/segmented_files/'+clip)\n",
    "\n",
    "            \"\"\" \n",
    "            masker.fit_transform(clip_segment_nii) will take the voxelwise mean (ie it preserves\n",
    "            timecourse information). This produces a timepoint x parcel csv, eg for physical's \n",
    "            clip_0, it will generate a (74,40) shape.\n",
    "            \"\"\"\n",
    "\n",
    "            time_by_parcel = masker.fit_transform(clip_segment_nii) #producess a timepoint x parcel csv, eg (2, 400)\n",
    "\n",
    "            single_clip_matrix_list.append(time_by_parcel)\n",
    "        \n",
    "        for clip in v2_clips:\n",
    "\n",
    "            sub = clip.split(\"_\")[0]\n",
    "            clip_segment_nii = nb.load(segmented_clip_path+sub+'/segmented_files/'+clip)\n",
    "\n",
    "            time_by_parcel = masker.fit_transform(clip_segment_nii) #producess a timepoint x parcel csv, eg (2, 400)\n",
    "\n",
    "            single_clip_matrix_list.append(time_by_parcel)\n",
    "        \n",
    "        single_clip_mat_arr = np.array(single_clip_matrix_list) #shape is (28, TR, 400) so for clip-2 its: (28,14,400)\n",
    "\n",
    "        for p in range(400):\n",
    "            sub_clip_corr = np.corrcoef(single_clip_mat_arr[:,:,p]) #(28,28)\n",
    "\n",
    "            neural_lower = nilearn.connectome.sym_matrix_to_vec(sub_clip_corr, discard_diagonal=True)\n",
    "            version_lower = nilearn.connectome.sym_matrix_to_vec(version_matrix, discard_diagonal=True)\n",
    "\n",
    "            result = np.corrcoef(neural_lower, version_lower)[0][1]\n",
    "            clip_to_version_correlation[ix,p] = result\n",
    "\n",
    "    output = pd.DataFrame(clip_to_version_correlation) #(12,400), 12 clips 400 parcels\n",
    "    output.to_csv(movie+\"_parcel_NIM_ISC.csv\")\n",
    "\n",
    "    #takes an hour to run locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Internal Monologues ISC Analyses</font> \n",
    "This is a secondary analysis. The primary hypothesis is about the non-internal monologues with regards to the intersubject correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_clip_path = '/Volumes/Scraplab/fSEND/inarr_data/'\n",
    "sublist = [x for x in os.listdir(segmented_clip_path) if 'sub' in x]\n",
    "complete_cliplist = []\n",
    "\n",
    "#compile all of the clip files across subjects\n",
    "for sub in sublist:\n",
    "    try:\n",
    "        segment_list = [x for x in os.listdir(segmented_clip_path+sub+os.sep+\"segmented_files/\") if \"_IM_\" in x] # [ IM / NIM ]\n",
    "        #discard_first_clip = [x for x in segment_list if \"_clip-0_\" not in x]\n",
    "\n",
    "        for f in segment_list:\n",
    "            complete_cliplist.append(f)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "movie_list = ['physical', 'stutterer']\n",
    "\n",
    "#we don't include the very first clip, clip-0:\n",
    "im_clips = ['clip-1','clip-3','clip-5','clip-7','clip-9','clip-11','clip-13',\n",
    "             'clip-15','clip-17','clip-19','clip-21','clip-23']\n",
    "\n",
    "for movie in movie_list:\n",
    "    #all the subject's clips corresponding to stutterer, for example\n",
    "    movie_cliplist = [x for x in complete_cliplist if movie in x]\n",
    "\n",
    "    clip_to_version_correlation = np.zeros([12,400]) #IM is 12 clips\n",
    "\n",
    "    for ix, i in enumerate(im_clips):\n",
    "        print(\"starting clip:\", i)\n",
    "\n",
    "        #all 28 subjects clip-1's for example\n",
    "        filtered_clips = [x for x in movie_cliplist if i+\"_\" in x]\n",
    "\n",
    "        #doing this ensures that my resulting matrix is in the *same exact order* as our version matrix\n",
    "        v1_clips = natsorted([x for x in filtered_clips if 'version-1' in x])\n",
    "        v2_clips = natsorted([x for x in filtered_clips if 'version-2' in x])\n",
    "\n",
    "        single_clip_matrix_list = [] #this will be len(28) for each subject!\n",
    "\n",
    "        for clip in v1_clips:\n",
    "\n",
    "            sub = clip.split(\"_\")[0]\n",
    "            clip_segment_nii = nb.load(segmented_clip_path+sub+'/segmented_files/'+clip)\n",
    "\n",
    "            \"\"\" \n",
    "            masker.fit_transform(clip_segment_nii) will take the voxelwise mean (ie it preserves\n",
    "            timecourse information). This produces a timepoint x parcel csv, eg for physical's \n",
    "            clip_0, it will generate a (74,40) shape.\n",
    "            \"\"\"\n",
    "\n",
    "            time_by_parcel = masker.fit_transform(clip_segment_nii) #producess a timepoint x parcel csv, eg (2, 400)\n",
    "            single_clip_matrix_list.append(time_by_parcel)\n",
    "        \n",
    "        for clip in v2_clips:\n",
    "\n",
    "            sub = clip.split(\"_\")[0]\n",
    "            clip_segment_nii = nb.load(segmented_clip_path+sub+'/segmented_files/'+clip)\n",
    "\n",
    "            time_by_parcel = masker.fit_transform(clip_segment_nii) #producess a timepoint x parcel csv, eg (2, 400)\n",
    "            single_clip_matrix_list.append(time_by_parcel)\n",
    "        \n",
    "        single_clip_mat_arr = np.array(single_clip_matrix_list) #shape is (28, TR, 400) so for clip-2 its: (28,14,400)\n",
    "\n",
    "        for p in range(400):\n",
    "            sub_clip_corr = np.corrcoef(single_clip_mat_arr[:,:,p]) #(28,28)\n",
    "\n",
    "            neural_lower = nilearn.connectome.sym_matrix_to_vec(sub_clip_corr, discard_diagonal=True)\n",
    "            version_lower = nilearn.connectome.sym_matrix_to_vec(version_matrix, discard_diagonal=True)\n",
    "\n",
    "            result = np.corrcoef(neural_lower, version_lower)[0][1]\n",
    "            clip_to_version_correlation[ix,p] = result\n",
    "\n",
    "    output = pd.DataFrame(clip_to_version_correlation) #(12,400), 12 clips 400 parcels\n",
    "    output.to_csv(movie+\"_parcel_IM_ISC.csv\")\n",
    "\n",
    "    #takes an hour to run locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='teal'>Plotting the Sanity Checks Analyses</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "from nilearn import plotting, datasets, image, surface\n",
    "import nibabel as nb\n",
    "from nilearn.plotting import plot_img_on_surf\n",
    "from nilearn.image import new_img_like\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fsaverage = datasets.fetch_surf_fsaverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='darkgoldenrod'>Non-internal monologues</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load observed values\n",
    "phys_isc, stut_isc = pd.read_csv('outputs/physical_parcel_NIM_ISC.csv', index_col=0), pd.read_csv('outputs/stutterer_parcel_NIM_ISC.csv', index_col=0)\n",
    "phys_isc_avg, stut_isc_avg = phys_isc.mean(axis=0).reset_index(drop=True), stut_isc.mean(axis=0).reset_index(drop=True)\n",
    "isc_avg = np.array((phys_isc_avg + stut_isc_avg) / 2) #shape (400)\n",
    "\n",
    "#load permutated values\n",
    "phys_perm = np.load('/Volumes/Scraplab/lindseytepfer/inarr/physical_isc_nim_permutation_matrix.npy') # (400, 10000)\n",
    "stut_perm = np.load('/Volumes/Scraplab/lindseytepfer/inarr/stutterer_isc_nim_permutation_matrix.npy')\n",
    "perm_avg = np.mean([phys_perm, stut_perm], axis=0) #shape (400, 10000)\n",
    "\n",
    "permuted_vals = np.abs(perm_avg)\n",
    "null_dist = permuted_vals.max(axis=0) #shape (10000) \n",
    "\n",
    "results_pvals = np.empty((400))\n",
    "\n",
    "for parcel in range(isc_avg.shape[0]): #400, for each parcel\n",
    "    results_pvals[parcel] = np.mean(null_dist >= abs(isc_avg[parcel]))\n",
    "\n",
    "significant = np.where(results_pvals < 0.05, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create correlation value image\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "\n",
    "#need to insert the 0 for proper indexing. \n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")\n",
    "\n",
    "atlas = nb.load(schaefer_atlas.maps) # .maps provides the path to the map\n",
    "atlas_data = atlas.get_fdata()\n",
    "\n",
    "mapped_data = np.zeros_like(atlas_data)\n",
    "mapped_data[atlas_data == 0] = np.nan # mark\n",
    "unique_regions = np.unique(atlas_data)[1:]\n",
    "\n",
    "for i, region in enumerate(unique_regions):\n",
    "    mapped_data[atlas_data == region] = isc_avg[i]\n",
    "\n",
    "nim_corr_img = nb.Nifti1Image(mapped_data, atlas.affine)\n",
    "nb.save(nim_corr_img, 'outputs/nim_corr_map.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corrected p-value mask:\n",
    "n_rois = 399\n",
    "\n",
    "roi_data = image.get_data(atlas)  # (x,y,z) array with ROI labels\n",
    "significant_mask = np.zeros_like(roi_data, dtype=np.float32)\n",
    "\n",
    "for roi_label in range(1, n_rois+1):\n",
    "    if significant[roi_label-1] == 1:  # ROI labels start at 1\n",
    "        significant_mask[roi_data == roi_label] = 1  # Tag significant voxels\n",
    "\n",
    "mask_img = new_img_like(atlas, significant_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results\n",
    "\n",
    "plotting.plot_img_on_surf(nim_corr_img,\n",
    "    \"fsaverage\", inflate=True,\n",
    "    views=['lateral', 'medial'],  # Hemispheric views to display\n",
    "    hemispheres=['left', 'right'],  # Both hemispheres\n",
    "    #threshold=0.05,  # Highlight significant p-values\n",
    "    cmap='seismic',  # Colormap (e.g., 'viridis', 'coolwarm')\n",
    "    colorbar=True,  # Show colorbar\n",
    "    mask_img=mask_img,\n",
    "    vmin=-1, vmax=1, #removing these options to let it be automatic\n",
    ")\n",
    "\n",
    "# fig = plt.gcf()\n",
    "# fig.savefig('isc_corrected.png', dpi=300, facecolor='w', bbox_inches='tight')\n",
    "# plt.close()\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='darkgoldenrod'>Internal monologues</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load observed values\n",
    "phys_isc, stut_isc = pd.read_csv('outputs/physical_parcel_IM_ISC.csv', index_col=0), pd.read_csv('outputs/stutterer_parcel_IM_ISC.csv', index_col=0)\n",
    "phys_isc_avg, stut_isc_avg = phys_isc.mean(axis=0).reset_index(drop=True), stut_isc.mean(axis=0).reset_index(drop=True)\n",
    "isc_avg = np.array((phys_isc_avg + stut_isc_avg) / 2) #shape (400)\n",
    "\n",
    "#load permutated values\n",
    "phys_perm = np.load('/Volumes/Scraplab/lindseytepfer/inarr/physical_isc_im_permutation_matrix.npy')\n",
    "stut_perm = np.load('/Volumes/Scraplab/lindseytepfer/inarr/stutterer_isc_im_permutation_matrix.npy')\n",
    "perm_avg = np.mean([phys_perm, stut_perm], axis=0) #shape (400, 10000)\n",
    "\n",
    "permuted_vals = np.abs(perm_avg)\n",
    "null_dist = permuted_vals.max(axis=0) #shape (10000) \n",
    "\n",
    "results_pvals = np.empty((400))\n",
    "\n",
    "for parcel in range(isc_avg.shape[0]): #400, for each parcel\n",
    "    results_pvals[parcel] = np.mean(null_dist[:] >= abs(isc_avg[parcel]))\n",
    "\n",
    "significant = np.where(results_pvals < 0.01, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create correlation image\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=17, resolution_mm=1,\n",
    "    data_dir=None, base_url=None, resume=True, verbose=1)\n",
    "\n",
    "#need to insert the 0 for proper indexing. \n",
    "schaefer_atlas.labels = np.insert(schaefer_atlas.labels, 0, \"Background\")\n",
    "\n",
    "atlas = nb.load(schaefer_atlas.maps) # .maps provides the path to the map\n",
    "atlas_data = atlas.get_fdata()\n",
    "\n",
    "mapped_data = np.zeros_like(atlas_data)\n",
    "mapped_data[atlas_data == 0] = np.nan # mark\n",
    "unique_regions = np.unique(atlas_data)[1:]\n",
    "\n",
    "for i, region in enumerate(unique_regions):\n",
    "    mapped_data[atlas_data == region] = isc_avg[i]\n",
    "\n",
    "im_corr_img = nb.Nifti1Image(mapped_data, atlas.affine)\n",
    "#nb.save(nim_corr_img, 'tvalue_map.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create corrected p-value mask:\n",
    "n_rois = 399\n",
    "\n",
    "roi_data = image.get_data(atlas)  # (x,y,z) array with ROI labels\n",
    "significant_mask = np.zeros_like(roi_data, dtype=np.float32)\n",
    "\n",
    "for roi_label in range(1, n_rois+1):\n",
    "    if significant[roi_label-1] == 1:  # ROI labels start at 1\n",
    "        significant_mask[roi_data == roi_label] = 1  # Tag significant voxels\n",
    "\n",
    "mask_img = new_img_like(atlas, significant_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results\n",
    "\n",
    "plotting.plot_img_on_surf(im_corr_img,\n",
    "    \"fsaverage\", inflate=True,\n",
    "    views=['lateral', 'medial'],  # Hemispheric views to display\n",
    "    hemispheres=['left', 'right'],  # Both hemispheres\n",
    "    #threshold=0.05,  # Highlight significant p-values\n",
    "    cmap='seismic',  # Colormap (e.g., 'viridis', 'coolwarm')\n",
    "    colorbar=True,  # Show colorbar\n",
    "    mask_img=mask_img,\n",
    "    vmin=-1, vmax=1, #removing these options to let it be automatic\n",
    ")\n",
    "\n",
    "# fig = plt.gcf()\n",
    "# fig.savefig('isc_corrected.png', dpi=300, facecolor='w', bbox_inches='tight')\n",
    "# plt.close()\n",
    "\n",
    "plotting.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
